{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08357d5-9c0c-40be-827a-7457e18fb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7315d7-8c41-4006-9f04-70b0501f37ac",
   "metadata": {},
   "source": [
    "## **IMPORTING THE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c00777-7eb7-4fcc-97c6-af02d49958a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\sudhanshu\\Downloads\\Prajwal Sonekar\\study_buddy_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f50ef2-0d8d-446a-a088-b1dca8b25959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87596, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc573ae-ff90-4931-a363-3897afe04f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87596 entries, 0 to 87595\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   topic       87596 non-null  object\n",
      " 1   context     87596 non-null  object\n",
      " 2   question    87596 non-null  object\n",
      " 3   answer      87596 non-null  object\n",
      " 4   difficulty  87596 non-null  object\n",
      " 5   attempted   87596 non-null  bool  \n",
      " 6   correct     87596 non-null  int64 \n",
      "dtypes: bool(1), int64(1), object(5)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef14e7b7-a895-4c48-b043-9f3d802f9f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic         0\n",
       "context       0\n",
       "question      0\n",
       "answer        0\n",
       "difficulty    0\n",
       "attempted     0\n",
       "correct       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b58d330-c463-4b4f-a1e4-ab1599530050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067cac-dfc1-4a92-bc96-5570df766a98",
   "metadata": {},
   "source": [
    "## **STUDY NOTES SUMMARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7165dbad-ae64-4945-84ac-65905f562d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def simple_summary(text, max_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return \" \".join(sentences[:max_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3e26eb-504d-4936-8bac-9748e38b5dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_context = df['context'].iloc[0]\n",
    "simple_summary(sample_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f8e38-0ad8-4ea1-9275-e0e371abd6ac",
   "metadata": {},
   "source": [
    "## **QUICK QUESTION ANSWERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd77257-d4d7-4eed-af28-89ff7232b394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    To whom did the Virgin Mary allegedly appear i...\n",
       "answer                             Saint Bernadette Soubirous\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['question', 'answer']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a8a05-4e5a-41e0-81f8-3ff6398144e0",
   "metadata": {},
   "source": [
    "## **GENERATE FLASHCARDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8b0b7d-d3d5-4894-80de-f664e529210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flashcard(index):\n",
    "    return {\n",
    "        \"Question\": df.loc[index, 'question'],\n",
    "        \"Answer\": df.loc[index, 'answer']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f6f8e4-0e55-4ad4-b79d-802b575572d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'Where is the headquarters of the Congregation of the Holy Cross?',\n",
       " 'Answer': 'Rome'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_flashcard(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00a04a-d095-49a9-b7f4-207297644f3e",
   "metadata": {},
   "source": [
    "## **QUIZ GENERATOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44fa3608-faaf-413b-8055-de2025e5cde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53546</th>\n",
       "      <td>Who was part of Hollywood Ten?</td>\n",
       "      <td>Herbert Biberman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13596</th>\n",
       "      <td>What division of NBCUniversal revived Gramercy...</td>\n",
       "      <td>Focus Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29659</th>\n",
       "      <td>Chinese troops attacked the UN forces when the...</td>\n",
       "      <td>the Yalu River</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10416</th>\n",
       "      <td>What Latin word does \"cardinal\" come from?</td>\n",
       "      <td>cardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21519</th>\n",
       "      <td>Which train line used to have an 18th Street S...</td>\n",
       "      <td>IRT Lexington Avenue Line</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "53546                     Who was part of Hollywood Ten?   \n",
       "13596  What division of NBCUniversal revived Gramercy...   \n",
       "29659  Chinese troops attacked the UN forces when the...   \n",
       "10416         What Latin word does \"cardinal\" come from?   \n",
       "21519  Which train line used to have an 18th Street S...   \n",
       "\n",
       "                          answer  \n",
       "53546           Herbert Biberman  \n",
       "13596             Focus Features  \n",
       "29659             the Yalu River  \n",
       "10416                      cardo  \n",
       "21519  IRT Lexington Avenue Line  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz = df.sample(5)[['question', 'answer']]\n",
    "quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13359f-c86f-47f7-9ceb-18bbb00f74f5",
   "metadata": {},
   "source": [
    "## **SUMMARIES FOR EXAM USE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af148d41-aba6-41ea-89f4-57006b8789e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def exam_focused_summary(text, max_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    freq = Counter(words)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentences:\n",
    "        for word in re.findall(r'\\w+', sent.lower()):\n",
    "            if word in freq:\n",
    "                sentence_scores[sent] = sentence_scores.get(sent, 0) + freq[word]\n",
    "\n",
    "    ranked = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "    return \" \".join(ranked[:max_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c88988b-20db-41a6-8e2d-c208a3941053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam_focused_summary(df['context'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96a799-5e48-4368-bf74-0f40d16f8dc9",
   "metadata": {},
   "source": [
    "## **TURN Q&A INTO FLASHCARDS PROPERLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92156d5a-daea-4f4d-bcd1-6c7e3bcbd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flashcard(index):\n",
    "    return {\n",
    "        \"Front (Question)\": df.loc[index, 'question'],\n",
    "        \"Back (Answer)\": df.loc[index, 'answer'],\n",
    "        \"Source\": \"Study Buddy AI\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebec838f-b82f-4ee8-9bf6-a166e508416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Front (Question)': 'What is the Grotto at Notre Dame?',\n",
       " 'Back (Answer)': 'a Marian place of prayer and reflection',\n",
       " 'Source': 'Study Buddy AI'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flashcard(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7e109-3007-4dd5-8ec1-1d9717e68ff6",
   "metadata": {},
   "source": [
    "## **QUIZ GENERATION (ADD DIFFICULTY TAGS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "503a9b49-7306-468c-858d-1253bd3bce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty_level(answer):\n",
    "    lenth = len(answer.split())\n",
    "    if lenth <= 3:\n",
    "        return \"Easy\"\n",
    "    elif lenth <= 8:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98ea54cc-239b-42b5-9563-589a7c17b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['difficulty'] = df['answer'].apply(difficulty_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876790d-9458-4969-bac0-8d83bd2ef480",
   "metadata": {},
   "source": [
    "## **GENERATE AN EXAM QUIZ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23740964-1a2f-4ab2-bc9c-96deba0f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz(level=\"Medium\", n=5):\n",
    "    quiz = df[df['difficulty'] == level].sample(n)\n",
    "    return quiz[['question', 'answer', 'difficulty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a841ba0-65ff-4606-bf7b-60032541f5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57394</th>\n",
       "      <td>Which disc could hold about three minutes of r...</td>\n",
       "      <td>10-inch</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82046</th>\n",
       "      <td>What is the phenomenon where a P-N junction em...</td>\n",
       "      <td>electroluminescence</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49779</th>\n",
       "      <td>What remained an important focus during the 19...</td>\n",
       "      <td>Communism</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16077</th>\n",
       "      <td>What particle is associated with the yellowing...</td>\n",
       "      <td>lignin</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245</th>\n",
       "      <td>Which year did the USSR cancel the N1 rocket p...</td>\n",
       "      <td>1976</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question               answer  \\\n",
       "57394  Which disc could hold about three minutes of r...              10-inch   \n",
       "82046  What is the phenomenon where a P-N junction em...  electroluminescence   \n",
       "49779  What remained an important focus during the 19...            Communism   \n",
       "16077  What particle is associated with the yellowing...               lignin   \n",
       "12245  Which year did the USSR cancel the N1 rocket p...                 1976   \n",
       "\n",
       "      difficulty  \n",
       "57394       Easy  \n",
       "82046       Easy  \n",
       "49779       Easy  \n",
       "16077       Easy  \n",
       "12245       Easy  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_quiz(\"Easy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc7ccc-7d14-4d68-877a-e9370165698f",
   "metadata": {},
   "source": [
    "## **WEAK-TOPIC TRACKING (VERY IMPORTANT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba81ed2f-48a4-432d-a726-b47422ac5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df['attempted'] = True\n",
    "df['correct'] = [random.choice([0,1]) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bc9cc46-16e3-4b9c-8be6-3ec33d84da6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy</td>\n",
       "      <td>0.497633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hard</td>\n",
       "      <td>0.499037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medium</td>\n",
       "      <td>0.488688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  difficulty  accuracy\n",
       "0       Easy  0.497633\n",
       "1       Hard  0.499037\n",
       "2     Medium  0.488688"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_performance = (\n",
    "    df.groupby('difficulty')['correct']\n",
    "    .mean()\n",
    "    .reset_index(name='accuracy')\n",
    ")\n",
    "\n",
    "topic_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11a0da8-6f93-48c7-9a9b-21ac52033930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'context', 'question', 'answer', 'difficulty', 'attempted',\n",
       "       'correct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b55f9af6-fc68-478c-b0de-721cde06ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"study_buddy_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
